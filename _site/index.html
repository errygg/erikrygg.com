<!DOCTYPE html>
<html>
  <head>
    <title>Erik R. Rygg – DevOps enthusiast and practitioner, bourbon drinker, dog lover, and family man</title>

    <meta charset="utf-8" />
<meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
<meta http-equiv='X-UA-Compatible' content='IE=edge'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>


<meta name="description" content="DevOps enthusiast and practitioner, bourbon drinker, dog lover, and family man">
<meta property="og:description" content="DevOps enthusiast and practitioner, bourbon drinker, dog lover, and family man" />

<meta name="author" content="Erik R. Rygg" />




    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="alternate" type="application/rss+xml" title="Erik R. Rygg - DevOps enthusiast and practitioner, bourbon drinker, dog lover, and family man" href="/feed.xml" />

  </head>

  <body>
    <div class="wrapper-masthead">
      <div class="container">
        <header class="masthead clearfix">
          <a href="/" class="site-avatar"><img src="https://avatars2.githubusercontent.com/u/1319170?s=400&u=118595ff64881edd04c8dcd9f44280e98558e7e9&v=4" /></a>

          <div class="site-info">
            <h1 class="site-name"><a href="/">Erik R. Rygg</a></h1>
            <p class="site-description">DevOps enthusiast and practitioner, bourbon drinker, dog lover, and family man</p>
          </div>

          <nav>
            <a href="/">Blog</a>
            <a href="/about">About</a>
          </nav>
        </header>
      </div>
    </div>

    <div id="main" role="main" class="container">
      <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" >

<div class="posts">
  <p>


</p>
  
  <article class="post">

    <h1>A Tale of Two Terraforms — A Model for Managing Immutable and Mutable Infrastructure</h1>

    <div class="entry">
      <p><p>Look, immutable infrastructure is awesome and if you haven’t looked into this deployment methodology you should really read <a href="https://blog.codeship.com/immutable-infrastructure/">this</a> article. <a href="https://blog.codeship.com/author/florianmotlik/">Florian Motlik</a> discusses what immutable really is (FYI, it’s not just containerization) and how to properly role in new changes atomically to minimize, or even eliminate, downtimes due to upgrades, patches, etc.</p><p>However, immutable isn’t always an option, in fact most enterprises take time to migrate to new architectures and it is often necessary to keep some mutable servers around until we can properly architect an atomic, blue-green, change process. What I want to propose is a way for you to build up immutable components while also maintaining some of the older mutable, managed instances, all with the help of Terraform and Packer!</p><h3>Terraform Your Infrastructure</h3><p>Ok, hopefully if you are reading this article you’ve at least heard of Terraform. I’m biased because I’m an avid user of Terraform, but it rocks as a solution for codifying your infrastructure and even your applications. If you haven’t played with the open source version, head over <a href="https://www.terraform.io/">here</a> and play with it a bit. It will probably change your life.</p><p>Those of you who have played with it may know how to build up some base infrastructure, like a VPC or VNet, add some subnets, routes, NACLs, etc. Once you get the base laid down though, you now have to bring up some instances. It could be a VPN, NAT, bastion host, or an application instance. Lets explore how you would use Terraform to configure a bastion host that you might use as an SSH gateway for users to get into your brand new VPC you built. We won’t go into the instance creation itself as we want to discuss configuring (or provisioning) instances.</p><h4>Terraform Provisioning</h4><p>A nice clean way to provide optional provisioning in your modules is to define a null_resource that will run the provisioning if the caller so chooses. Here is an example:</p><pre>resource &quot;random_id&quot; &quot;bastion&quot; {<br>  byte_length <strong>=</strong> 8<br>}</pre><pre>resource “null_resource” “run_chef” {</pre><pre>  # Provision nodes with chef if chef is enabled<br>  count = “${var.chef_enabled ? var.node_count : 0}”</pre><pre>  provisioner “chef” {</pre><pre>    environment = “${var.environment}”<br>    version         = “${var.chef_version}”<br>    server_url      = “${var.chef_server_url}”<br>    recreate_client = true<br>    user_name       = “${var.bootstrap_user}”<br>    user_key        = “${file(var.bootstrap_pem)}”</pre><pre>    # Run list is based off of `name_prefix` AWS tag<br>    run_list = [“role[${var.environment}]”]</pre><pre>    # Unique node name using random_id resource<br>    node_name = “bastion-${var.environment}-${element(random_id.bastion.*.hex, count.index)}”<br>  }<br>}</pre><p>Ok, the big bit here that we’ve done to make this a <em>mutable</em> instance is that we’ve added a <em>provisioner</em> block. This is fine, but we have pushed the main configuration to the deployment of this instance. Now there are a couple things to note here. Terraform provisioners are only run when the resource is created, not every time that Terraform is run. So, that means you’ll have to manage the continual run of chef using a cron job or run chef as a service. Also, if the provision fails (i.e. cookbook fails) then the terraform run will fail as well. Rerunning terraform will actually just recreate the entire resource again and provision again.</p><p>Now this is fine and good, we’re going to let our team start using this bastion while we build up the rest of the infrastructure.</p><p>(Months go by and immutable enters the picture)</p><h4>Packer Provisioning then Terraform the Infrastructure</h4><p>Now that we have an operational environment we’ve decided to change it on it’s head and go immutable. We are going to start with immutable VMs. Well, there’s immediately a problem. That bastion host we built up months ago is now a special snowflake that if we make any mods to it, the dev team will cry foul, schedules will push, and basically our heads will be displayed on pikes. So, instead of deal with that, we are going to start rolling out <em>new </em>bastion hosts that folks can migrate to. These are going to be built with <a href="http://packer.io">Packer</a> and configured well in advance of the deployment. With Packer we will create a <em>custom</em> AMI/VHD/Image for us to use in the deployment. The fundamental difference here is that we’ve moved the provisioning from deployment way back to the left during the development phase. What? There’s a “development phase” for infrastructure? Yes!</p><p>If we take what we did in Terraform above and pull it into a Packer JSON template, it would look like:</p><pre>{<br>  &quot;variables&quot;: {<br>    &quot;environment&quot;: &quot;{{env `PACKER_CHEF_ENV`}}&quot;,<br>    &quot;chef_server_url&quot;: &quot;{{env `PACKER_CHEF_URL`}}&quot;<br>    &quot;bootstrap_user&quot;: &quot;{{env `PACKER_CHEF_BOOTSTRAP_USER`}}&quot;,<br>    &quot;bootstrap_pem&quot;: &quot;{{env `PACKER_CHEF_BOOTSTRAP_PEM`}}&quot;<br>  },</pre><pre>  &quot;builders&quot;: [ ... ],</pre><pre>  &quot;provisioners&quot;: [<br>    {<br>      &quot;type&quot;: &quot;chef-client&quot;,<br>      &quot;chef_environment&quot;: &quot;{{ user `environment` }}&quot;,<br>      &quot;server_url&quot;: &quot;{{ user `chef_server_url` }}&quot;,<br>      &quot;validation_client_name&quot;: &quot;{{ user `bootstrap_user` }}&quot;,<br>      &quot;validation_key_path&quot;: &quot;{{ user `bootstrap_pem` }}&quot;,</pre><pre>&quot;run_list&quot;: [<br>        &quot;role[{{ user `environment` }}]&quot;<br>      ],<br>      &quot;node_name&quot;: &quot;bastion-{{ user `environment` }}&quot;<br>    }<br>  ]<br>}</pre><p>Once the new image is built with Packer, we can then just reference the new image ID and forego the provisioning phase all together in our Terraform code. This makes for a much cleaner and easily maintainable infrastructure.</p><h3>Mutable and Immutable Living Side-by-Side</h3><p>Ok, so the above example is best case. I’ll admit it is difficult to migrate from mutable to immutable. That snowflake instance we built months ago may need to live on for quite some time, so we’ll need to manage it via some configuration management tool until we can convince the dev team they need to move to the new immutable bastion. This is going to be the majority of cases when migrating to immutable architecture. It will be a process of going service by service and doing the blue-green deployment: spin up the new immutable stuff, switch a load balancer over, make sure everything is cool, then shut down that old busted mutable cluster.</p><p>This migration sounds challenging and hard, but once you’ve done it once the migrations become easier and easier and eventually blue-green deployments will be a breeze. Rolling in new changes is easier, faster, and safer. Infrastructure development can include all sorts of goodies like policy enforcement (these instances need these tags), infrastructure testing, throughput testing, etc. And all this can happen well in advance of deployment which means your infrastructure is now able to accept safe, tested, and resilient change.</p><p>Go forth and change my friends!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=fa0f5422c27b" width="1" height="1"><hr><p><a href="https://medium.com/rigged-ops/a-tale-of-two-terraforms-a-model-for-managing-immutable-and-mutable-infrastructure-fa0f5422c27b">A Tale of Two Terraforms — A Model for Managing Immutable and Mutable Infrastructure</a> was originally published in <a href="https://medium.com/rigged-ops">Rigged Ops</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p></p>
    </div>

  </article>
  
  <article class="post">

    <h1>HashiCorp Vault and the Magic of Consul Templates</h1>

    <div class="entry">
      <p><h4>Using Terraform to Spin up Vault/Consul and Pull Secrets with consul-template</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*MFI0A3dkVjixTwdYZLqEDw.jpeg" /></figure><p>Ok, so I know I said I’d talk more about how to sign ssh keys using the Vault CA, but I got side tracked with how awesome Consul is and how to integrate it with Vault. I also wanted to build a <em>real</em> Vault instance with a <em>real</em> Consul backend and didn’t want to do it in AWS because I’m cheap (and on a plane often). So I built up a production-worthy stack locally with Docker on my Mac and it works like a champ! So stoked to show you how it works, but it did get a little involved.</p><p>The basic architecture is relatively simple. I spun up 3 Docker containers — a vault server, a consul backend server, and a vault/consul-template client. As much as I’d like to say it was a simple task, it got a bit complicated. Not to mention I also decided to throw in Terraform to codify the build but also kitchen-terraform to test it all out. Phew! That’s a lot of HashiCorp tech all wrapped into a nice little package.</p><p>To start out, here are the technologies utilized in this example:</p><ul><li><a href="https://newcontext-oss.github.io/kitchen-terraform/tutorials/docker_provider.html">kitchen-terraform</a></li><li><a href="https://www.terraform.io/docs/providers/docker/index.html">Terraform</a></li><li><a href="https://www.docker.com/docker-mac">Docker for Mac</a></li><li><a href="https://www.vaultproject.io/docs/index.html">Vault</a></li><li><a href="https://www.consul.io/docs/index.html">Consul</a></li><li><a href="https://github.com/hashicorp/consul-template#consul-template">consul-template</a></li></ul><p>If you haven’t used kitchen-terraform before, you’re in for a treat. It’s a sweet tool that allows you to build test suites for your Terraform modules and use Inspec to validate your deployments. We won’t be doing the Inspec validation in this blog, but kitchen-terraform allows us an easy way to test out a live configuration with the (relatively) generic modules.</p><p>Another item to note here is that Vault really isn’t meant to be completely automated. There are some security decisions made in the design that make it difficult, if not impossible, to completely automate the deployment AND configuration. Specifically, where I found the biggest hurdle here was with authenticating the consul-template client. AFAIK, the only way to authenticate the consul-template client is to use a non-root token, and automating a user token became very difficult. Mitchell Hashimoto himself discussed in this Github issue the security implications of doing so. So, with that, we do have a couple manual bits and pieces we’ll run to get this stack up and running in a (relatively — non-TLS, but that’s the only bit I skimped on) production-ready deployment.</p><h3>Here We Go!</h3><p><a href="https://github.com/errygg/vault-integration-examples.git">This</a> is the GitHub project I built to accomplish all this awesomeness, so you can play along!</p><h4>Step 1 — Setup the Initial Stack</h4><p>In that project, navigate to vault-integration-exmamples/consul-template/modules/setup_stack.</p><p>Run bundle exec kitchen converge. This will run test-kitchen with the kitchen-terraform driver. You’ll see lots of stuff happening so let’s break it down.</p><p>kitchen-terraform is a test driver for testing out Terraform code. This project is constructed using Terraform to build up the vault, consul, and client (with consul-template). We use Docker networking to network all these containers together. The Vault and Consul Docker containers are just using the Docker Hub images available from HashiCorp. The client container is built up using a base ssh container. In the Terraform code, I use a combination of berkshelf and chef-zero as well as some good ol’ scripting to install consul-template, the Vault and Consul binaries, and the associated configuration.</p><p><em>There’s a bit of work-around-ary in my repo and if you want to get the reasons why, please send me a note.</em></p><p>The outputs include the internal Docker network IP addresses and hostnames for the containers.</p><p>Now, because of the way the root token is created and output with Vault, it’s output into a file in vault-integration-examples/consul-template/modules/setup_stack/tmp/vault_root_token.txt.</p><h4>Step 2 — Store the Vault Root Token</h4><p>Terraform allows you to store information in environment variables and use them in your modules. So, we’ll do that here with the root token. Run the following on your localhost:</p><pre>export TF_VAR_root_token=&lt;root_token&gt;<br>export VAULT_ADDR=https://localhost:8200</pre><h4>Step 3 — Configure the Client</h4><p>Now we’ll configure Vault with the Vault Terraform provider. The Vault container exports the service port to the localhost, so we’ll be able to configure the Vault policies and backends locally. Now you can just run bundle exec kitchen converge in vault-integration-examples/consul-template/modules/config_stack.</p><p>Unfortunately, we’re done with the Terraform work now. It’s all manual from here.</p><h4>Step 4 — Get the User Token</h4><p>Now the user has been configured in Vault using the userpass backend, we’ll login (we can run this locally) with that username/password and get the token:</p><pre>vault login -method=userpass username=myusername</pre><p>Use mypassword as the password and save off the token.</p><h4>Step 5 — SSH into the Client and Run consul-template</h4><p>Now let’s start working with the client so we can actually show how consul-template will work.</p><p>ssh now into the client:</p><pre>ssh root@localhost -p 2222</pre><p>and use root as the password. Take a look at the consul-template we’ll be using. The file is in /root/sectets.txt.tpl. This is a simple consul-template file where .Data.myvalue will be replace by the secret we have mapped in Vault. consul-template will not allow you to use root tokens to render files, so this is why we’ve setup a username/password without root policies. Lets export the user token so we can login to Vault with a user token:</p><pre>export VAULT_TOKEN=&lt;user_token&gt;</pre><p>We already have consul-template configured, so we can simply run:</p><pre>consul-template -once -config=/root/consul_template_config.json</pre><p>We are running -once so we don’t run consul-template as a daemon.</p><h4>Step 6 — Check out the consul-template File</h4><p>Take a look at /root/secrets.txt and you should see mysecret actually put into the file where .Data.myvalue was in the .tpl file! Nice job, you did it. There’s a ton of stuff you can do with Consul templates, but this is just an example of how to do it with a production-level Vault/Consul setup.</p><p>Stay tuned for the next iteration of my Vault journey!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=d4a053e7a4cc" width="1" height="1"><hr><p><a href="https://medium.com/rigged-ops/hashicorp-vault-and-the-magic-of-consul-templates-d4a053e7a4cc">HashiCorp Vault and the Magic of Consul Templates</a> was originally published in <a href="https://medium.com/rigged-ops">Rigged Ops</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p></p>
    </div>

  </article>
  
  <article class="post">

    <h1>Building a Local HashiCorp Vault Cluster</h1>

    <div class="entry">
      <p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*9Fea6EaEbDtRaZ9vIefzgQ.jpeg" /></figure><h3>Building a Local HashiCorp Vault Cluster — Volume 1</h3><h4>Let the infrastructure team figure out the deployment while we build out the policies</h4><p><a href="https://www.vaultproject.io/">HashiCorp Vault</a> is a sweet little product that can do all sorts of super cool, super secret management, but a production-level deployment is a bit of a task. Depending on how you want to store your secrets and how fault tolerant you want your clusters to be, it could take upwards of 8 or more instances (be it container, virtual machine, etc.). And that will just get you a Vault to work with, then there’s a daunting task of building policies, integrating backends, adding audits… my head hurts already.</p><p>In order to tackle this problem it makes sense to parallelize a little bit here. Let the infrastructure folks figure out the best way to architect and manage the deployment while we figure out how the heck Vault actually works within our organization. In order to do that easily, we can just spin up a simple local environment and get to work right away on our policy buildout.</p><h4>Deploy Dev Server Instance and Setup a Client</h4><p>Firstly, we are going to deploy a couple Docker containers, so we need to setup a Docker network to get these containers talking to each other. Assuming you already have Docker setup on your dev machine (we’re using Docker for Mac) it’s as simple as:</p><pre>MY-MAC$ docker network create vault-net<br>&lt;vault-net ID&gt;</pre><p>Alrighty, now we have the network we can use to connect our containers.</p><p>To get the Vault server instance running, we’ll pull down <a href="https://hub.docker.com/r/_/vault/">HashiCorp’s Vault</a> image and run it:</p><pre>MY-MAC$ docker pull vault</pre><pre>MY-MAC$ docker run --network vault-net --cap-add=IPC_LOCK -e &#39;VAULT_DEV_ROOT_TOKEN_ID=my_root_token_id&#39; -p 8200:8200 vault</pre><pre>&lt;Logs and stuff will show up here&gt;</pre><p>Note we are also connecting to the vault-net network. In a separate terminal (ensure you have the vault <a href="http://brewformulas.org/Vault">binary</a> installed on your local machine) run:</p><pre>MY-MAC$ export VAULT_ADDR=http://0.0.0.0:8200</pre><pre>MY-MAC$ export VAULT_TOKEN=my_root_token_id</pre><pre>MY-MAC$ vault status<br>Seal Type: shamir<br>Sealed: false<br>Key Shares: 1<br>Key Threshold: 1<br>Unseal Progress: 0<br>Unseal Nonce:<br>Version: 0.9.1<br>Cluster Name: vault-cluster-b631b373<br>Cluster ID: 2814a88c-4074-9122-3f9d-f5e81d7e8fc1</pre><pre>High-Availability Enabled: false</pre><p>Sweet! We have a dev Vault instance running in a container and we can connect to our instance outside the container. Now what?</p><p>We’re going to build another container that we can use as a Vault client to connect via SSH using a One Time Password (OTP).</p><h4>One Time SSH Passwords</h4><p>Ok, now we are going to setup our vault instance to deal with ssh using an OTP. We use this as a way to manage shared users such as the default ubuntu user on our ephemeral instances. This gives us a way to still ssh onto our nodes without propagating all the users to the nodes.</p><p><strong>Server Setup<br></strong>Firstly, we’ll need to mount the backend on our local machine:</p><pre>MY-MAC$ vault mount ssh<br>Successfully mounted &#39;ssh&#39; at &#39;ssh&#39;!</pre><p>Now we need a role to provide OTP to the clients. We’ll allow the ubuntu user on our client node to use this role.</p><pre>MY-MAC$ vault write ssh/roles/otp_role key_type=otp  default_user=ubuntu cidr_list=172.18.0.0/16</pre><p><strong>Client Setup<br></strong>We’ll need a node we can use to ssh to for our test, so let’s go ahead and setup a simple docker ssh container (note: this is a container I built specifically for this demo, it’s just a sshd container with sshd/PAM configured as well as vault-ssh-helper installed and you can check out the internals <a href="https://github.com/errygg/docker-vault-ssh-helper">here</a>). First, prune them since we are explicitly naming this container.</p><pre>MY-MAC$ docker container prune -f<br>Total reclaimed space: XB</pre><pre>MY-MAC$ docker run -d -P --name vault_ssh_client --network vault-net errygg/vault-ssh-helper</pre><pre>MY-MAC$ docker port vault_ssh_client<br>22/tcp -&gt; 0.0.0.0:&lt;random_port&gt;</pre><p>Cool, now we’ve got a client running attached to our vault-net network. But, the container is configured with PAM to connect to Vault and we don’t have any local users that are able to authenticate with local passwords. So we’ll use the ubuntu user to authenticate with the vault service.</p><p>Docker networking defaults to using the 172.18.0.0/16 subnet, so the vault instance should be 172.18.0.2 and the client should be 172.18.0.3.</p><p>To get the client to connect to Vault with ubuntu set the same env vars we did earlier and ssh to the box from our localhost. First, get the OTP:</p><pre>MY-MAC$ vault write ssh/creds/otp_role ip=172.18.0.3<br>Key             Value<br>---             -----<br>lease_id        ssh/creds/otp_role/&lt;ID&gt;<br>lease_duration  768h0m0s<br>lease_renewable false<br>ip              172.18.0.3<br>key             &lt;Password&gt;<br>key_type        otp<br>port            22<br>username        ubuntu</pre><p>Now try to ssh using the &lt;Password&gt;</p><pre>MY-MAC$ ssh ubuntu@localhost -p &lt;random port&gt;<br>Password: &lt;Password&gt;<br>Welcome to Ubuntu 14.04 LTS (GNU/Linux 4.4.0-101-generic x86_64)</pre><pre>* Documentation:  <a href="https://help.ubuntu.com/">https://help.ubuntu.com/</a></pre><pre>The programs included with the Ubuntu system are free software;<br>the exact distribution terms for each program are described in the<br>individual files in /usr/share/doc/*/copyright.</pre><pre>Ubuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by<br>applicable law.</pre><pre>ubuntu@a7037c6c8dae:~$</pre><p>Nice! We’ve got OTP working with ubuntuusing Vault roles/creds and the vault-ssh-helper! Pretty cool, but even cooler would be using Vault as a <a href="https://www.vaultproject.io/docs/secrets/ssh/signed-ssh-certificates.html">certificate authority</a> to store and distribute signed ssh keys for individual users. Stay tuned, we’ll explore that in the next iteration of my Vault journey!</p><p>Image Credit: <a href="https://www.freeimages.com/photographer/kikko77-32856">Kristian Hoffer</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=5575fe322a17" width="1" height="1"><hr><p><a href="https://medium.com/rigged-ops/building-a-local-hashicorp-vault-cluster-5575fe322a17">Building a Local HashiCorp Vault Cluster</a> was originally published in <a href="https://medium.com/rigged-ops">Rigged Ops</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p></p>
    </div>

  </article>
  

  <!--  -->
</div>

    </div>

    <div class="wrapper-footer">
      <div class="container">
        <footer class="footer">
          <a href="mailto:errygg@gmail.com"><i class="svg-icon email"></i></a>
<a href="https://github.com/errygg"><i class="svg-icon github"></i></a>
<a href="https://www.linkedin.com/in/errygg"><i class="svg-icon linkedin"></i></a>
<a href="https://www.twitter.com/errygg"><i class="svg-icon twitter"></i></a>
<a href="https://www.facebook.com/erygg"><i class="svg-icon facebook"></i></a>
<a href="https://instagram.com/errygg"><i class="svg-icon instagram"></i></a>

        </footer>
      </div>
    </div>
  </body>
</html>
